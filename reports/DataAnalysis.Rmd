---
title: "Exploratory data analysis"
author: "Alexandre Nanchen"
date: "23 April 2016"
output:
  html_document:
    fig_caption: yes
    keep_md: yes
  pdf_document: default
kfigr: prefix:True
---

```{r echo=FALSE, results="hide", message=FALSE}
    require(knitr)
    library(kfigr)
    require(dplyr)
    require(ggplot2)
    require(gridExtra)
    require(graphics)
    require(tm)
    require(wordcloud)
    require(RWeka)
    
    opts_knit$set(root.dir="../")
    opts_chunk$set(echo=TRUE)
    
```

```{r echo=FALSE}
readSample <- function(fileName) {
    cat("Reading ", fileName,"\n")
    df <- tbl_df(read.table(fileName,allowEscapes = T)) %>%
             rename(text=V1, wordCount = V2)
    return(df)
}
```

# Synopsis
Around the world, people are spending an increasing amount of time on their mobile devices for email,
social networking, banking and a whole range of other activities. But typing on mobile devices can be
a serious pain. 

The goal of the project is to build a predictive algorithm that help users by suggesting words as they are
typing.

# Data selection
The data is made available from the course web site. Here is a summary of the downloaded english US
text.

### Data statistics
```{r, echo=F}
tw <- strsplit(system("wc ../data/en_US/en_US.twitter.txt", intern = T), " +")
bl <- strsplit(system("wc ../data/en_US/en_US.blogs.txt", intern = T)," +")
nw <- strsplit(system("wc ../data/en_US/en_US.news.txt", intern = T)," +")
totalLines <- as.integer(tw[[1]][2]) + as.integer(bl[[1]][2]) + as.integer(nw[[1]][2])
totalWords <- as.integer(tw[[1]][3]) + as.integer(bl[[1]][3]) + as.integer(nw[[1]][3])
```

```{r, echo=F}
df <- data.frame(DataSet=c("Twitter", "News", "Blogs"), 
                 Number.lines=c(as.integer(tw[[1]][2]),as.integer(bl[[1]][2]), as.integer(nw[[1]][2])),
                 Number.words=c(as.integer(tw[[1]][3]),as.integer(bl[[1]][3]),as.integer(nw[[1]][3])))
kable(df,row.names = F, format.args=list(big.mark = "'"))
```

In total, **`r format(totalLines,big.mark="'")`** lines **`r format(totalWords,big.mark="'")`** words.

### Data sampling
The purpose of the data sampling is to select a representative subset of the 100 Mio words trying to avoid a potential
bias. This step is necessary because the final models will be running on a webserver with limited memory and disk
space resources.

A divide and conquer method associated with random sampling has been chosen:

1. Load all sentences having more than 3 words and less than 50 words.
2. Choose the total amount of lines for the subset, in our case 1.5 Mio (500'000 per source)
3. Subsample sentences from the available sources: twitter, blogs and news using the following algorithm:
    - For the given data source, compute sentence words length density
    - Use this density to randomly select the appropriate amount of sentences for each class

In total, **`r format(1499935,big.mark="'")`** lines **`r format(31213036,big.mark="'")`** words.

# Data preparation
Before modeling the language, some data transformations have been applied to the sampled data.

##### Special characters replacements:
   - Emoticons
   - Control characters
   - Special utf-8 characters
   
##### Text normalization:
   - Lowercasing
   - Numbers removal
   - Punctuation removal
   - White space normalization,
   - Offensive words removal

# Exploratory data analysis
```{r, echo=F}
CLEANDIR <- "../clean/en_US"
sentences <- c()
for (src in c("twitter","news","blogs")) {
    fileName <- sprintf("%s.%s.txt", "en_US", src)
    srcFile <- sprintf("%s/%s", CLEANDIR, fileName)
    df <- tbl_df(read.table(srcFile,allowEscapes = T, stringsAsFactors = F)) %>% rename(text=V1)
    sentences <- c(sentences, df$text)
}
```
The exploratory data analysis will be performed on **`r format(length(sentences),big.mark="'")`** sentences.

### N gram frequencies
#### Unigrams
```{r, echo=F}
unigramTokenizer <- function(x) NGramTokenizer(x, Weka_control(min = 1, max = 1))
unigrams <- unlist(lapply(sentences, unigramTokenizer))
nbUnigrams <- length(table(unigrams))
```
In total, there are **`r format(nbUnigrams, big.mark="'")`** unique words.

Here is a word cloud of words occurring more than 50 times.
```{r, echo=F, fig.height=4}
c <- Corpus(VectorSource(paste(sentences,collapse=" ")))
td <- DocumentTermMatrix(c)
wordcloud(c,scale=c(6,1), rot.per=0.35, min.freq=50, use.r.layout=F,
          colors= brewer.pal(8,"Spectral"), max.words = 100)
```

#### Bigrams and trigrams
```{r, echo=F}
bigramTokenizer <- function(x) NGramTokenizer(x, Weka_control(min = 2, max = 2))
bigrams <- unlist(lapply(sentences, bigramTokenizer))
nbBigrams <- length(table(bigrams))
trigramTokenizer <- function(x) NGramTokenizer(x, Weka_control(min = 3, max = 3))
trigrams <- unlist(lapply(sentences, trigramTokenizer))
nbTrigrams <- length(table(trigrams))
```
Here are some frequencies plot:
```{r, echo=F, fig.height=3, fig.width=9}
dfBigramFreq <- tbl_df(data.frame(table(bigrams))) %>% arrange(Freq) %>%
                mutate(bigrams = factor(bigrams,levels=as.character(bigrams)))

dfTrigramFreq <- tbl_df(data.frame(table(trigrams))) %>% arrange(Freq) %>%
                 mutate(trigrams = factor(trigrams,levels = as.character(trigrams)))
gbigram <- ggplot(tail(dfBigramFreq,10),aes(x=bigrams,y=Freq, fill=Freq)) + 
           geom_bar(stat="identity") + coord_flip() + theme_bw() + ggtitle("Most common bigrams")
gtrigram <- ggplot(tail(dfTrigramFreq,10),aes(x=trigrams,y=Freq, fill=Freq)) + 
           geom_bar(stat="identity") + coord_flip() + theme_bw() + ggtitle("Most common trigrams")
grid.arrange(gbigram,gtrigram,ncol=2)
```

### N-gram coverage
It can be interesting to relate the increase of unique ngrams and their decrease of coverage.

By coverage we mean the percentage of observed ngram versus all possible ngrams. For example
if there are 5 unigrams, the maximum number of bigram is 25 and if we have observered 5 unique bigrams, the bigram coverage is 5 for 25 or 20%.

```{r, echo=F, message=F, warning=F, fig.height=3, fig.width=9}
df <- data.frame(gram=c(1,2,3), count=c(nbUnigrams,nbBigrams,nbTrigrams), 
                 log10.coverage=c(as.integer(0), log10(nbBigrams/nbUnigrams^2), log10(nbTrigrams/nbUnigrams^3)))

p1 <- ggplot(df, aes(gram,count, fill=count)) + geom_bar(stat = "identity") + 
       theme_bw() + ggtitle("N-gram counts") + xlab("n-gram")
p2 <- ggplot(df, aes(gram,log10.coverage, fill=log10.coverage)) + geom_bar(stat = "identity") + 
           theme_bw() + ggtitle("N-gram coverage in log10") + xlab("n-gram")

grid.arrange(p1,p2,ncol=2)
```

We can se that as the number of unique ngram increase in linear fashion, the coverage decrease in an exponential manner.

### Word selection
We have seen that the number of unigram is very important when moving to higher orders like bigram and trigrams.

Basically we would like to reduce the number of unigrams while keeping the maximum information.
One way to check that is to order unigram by decreasing frequencies, find a cut-off point, take all unigram above that point, sum their total of occurence and compare that to the total word occurences.

```{r, echo=F, warning=F}
dfFreq <- tbl_df(data.frame(table(unigrams))) %>% arrange(desc(Freq))
dfFreq$coverage <- cumsum(dfFreq$Freq)/sum(dfFreq$Freq)
ggplot(dfFreq, aes(Freq, coverage)) + geom_point() + theme_bw() + xlim(0,50) + ggtitle("Influence of a count class removal") + xlab("Count classes")
```

On the plot we can see that the removal of words appearing only once will keep the words coverage around 90%.

### Conclusion
The first important conclusion is that bigram and trigram coverage are very low. It is indeed very difficult to observe all unigram combinations and we need a lot of text. So a strategy to combine multiple model of different orders need to be used. For exampling backing of to lower order models.

The second observation is that we can eliminate some keywords appearing rarely while keeping a coverage around 90%. Reducing the number of keywords will help to have a model of resonable size with a good predictive quality.


